# LLM Inference Optimization - Experimental Results

Generated: 2026-01-05 19:31:04

## Quantization

Total runs: 3

Average tokens/sec: 11.96

## Context Length

Total runs: 6

Average tokens/sec: 13.96

## Thread Scaling

Total runs: 24

Average tokens/sec: 12.25

## Memory Config

Total runs: 2

Average tokens/sec: 12.41

## Model Size

Total runs: 1

Average tokens/sec: 10.52


## Next Steps

- Analyze CSV files for detailed patterns
- Create visualizations
- Update report with findings
